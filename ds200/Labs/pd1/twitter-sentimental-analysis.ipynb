{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "6c70e441972baa6c7ac430f67dbc5da8727c2f48",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First my program will install all the required libraries and will read the csv file and will convert it to dataframe df.\n",
    "# It will give column names to the datafram df and then it will drop some columns except text and sentiment.\n",
    "# It will count the tweets group by sentiment.\n",
    "# It will add new column pre_clean_len to dataframe which is length of each tweet.\n",
    "# plot pre_clean_len column.\n",
    "# check for any tweets greater than 140 characters.\n",
    "# for each text i am calling tweet_cleaner function which will remove convert words to lower case, remove URL, remove hashtag, remove @mentions, HTML decoding, UTF-8 BOM decoding and converting words like isn't to is not.\n",
    "# And all this it will store in list called clean_tweet_texts.\n",
    "# Again it will tokenize the tweets in clean_tweet_texts and will do lemmatizing for every word in  list and after lemmatization it will join all the wirds again and will store it in new list called clean_df1.\n",
    "# This clean_df1 is then converted to dataframe and a sentiment column is added to it which is from old dataframe df.\n",
    "# Again it will add new column pre_clean_len to dataframe which is length of each tweet.\n",
    "# Again check for any tweets greater than 140 characters.\n",
    "# All the tweets is given to new variable x.\n",
    "# All the tweets sentiments is given to new variable y and plot the see shaoe of both x and y variable.\n",
    "# Now split the dataset in ratio 80:20 whereas 80% is for training and 20% is for testing.\n",
    "# Split both the x and y variables.\n",
    "# make a new instance vect of Tf-idf vectorizer and pass parameter as analyzer = \"word\" and ngrams_range = (1,1).\n",
    "# this ngrams_range is for feature selection is given (1,1) it will only select unigrams, (2,2) only bigrams, (3,3) only trigrams, (1,2) unigrams and bigrams, (1,3) unigrams, bigrams and trigrams.\n",
    "# we can also remove stop words over here by simply add new parameter stop_words = 'english'.\n",
    "# fit or traing data tweets to vect.\n",
    "# transform our training data tweets.\n",
    "# transform our testing data tweets.\n",
    "# import naive bayes and make object of it. Fit our traing tweets data and training tweets sentiment to the model.\n",
    "# do 10- fold cross validation on the training data and  calculate the mean accuracy of it.\n",
    "# predict the sentiments of testing tweets data.\n",
    "# calculate the accuracy of predicted sentiments with the original tweets sentiment of testing data.\n",
    "# plot the confusion matrix between original testing sentiment data and predicted sentiment.\n",
    "# import logistic regression and make object of it. Fit our traing tweets data and training tweets sentiment to the model.\n",
    "# do 10- fold cross validation on the training data and  calculate the mean accuracy of it.\n",
    "# predict the sentiments of testing tweets data.\n",
    "# calculate the accuracy of predicted sentiments with the original tweets sentiment of testing data.\n",
    "# plot the confusion matrix between original testing sentiment data and predicted sentiment.\n",
    "# import SVM and make object of it. Fit our traing tweets data and training tweets sentiment to the model.\n",
    "# do 10- fold cross validation on the training data and  calculate the mean accuracy of it.\n",
    "# predict the sentiments of testing tweets data.\n",
    "# calculate the accuracy of predicted sentiments with the original tweets sentiment of testing data.\n",
    "# plot the confusion matrix between original testing sentiment data and predicted sentiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "7f7b26518aef4c074f01a6f34982c020365378e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd #import pandas\n",
    "import numpy as numpy #import numpy\n",
    "from sklearn.utils import shuffle # to shuffle the data \n",
    "import random # import random\n",
    "import sklearn # import sklearn\n",
    "import nltk # import nltk\n",
    "from nltk.corpus import stopwords #import stop words\n",
    "import re # import regular expression\n",
    "from nltk.tokenize import word_tokenize # import word_tokenize\n",
    "import matplotlib\n",
    "import gensim\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "import unicodedata as udata\n",
    "import string\n",
    "import matplotlib.pyplot as plt #import matplotlib.pyplot \n",
    "df = pd.read_csv(\"../input/training.1600000.processed.noemoticon.csv\", encoding='latin-1', header=None) #read csv file without header as dataframe\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #  import TF-idf vectorizer\n",
    "df = shuffle(df) # shuffle csv file\n",
    "#tweets1 = df.iloc[0:9999,]\n",
    "#tweets1.to_csv('tweets1.csv', sep=',')\n",
    "%matplotlib inline\n",
    "#data\n",
    "print(sklearn.__version__)\n",
    "print(matplotlib.__version__)\n",
    "print(numpy.__version__)\n",
    "print(pd.__version__)\n",
    "print(nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "12d387bfafc9b3e0c401573a8935c30cb73b5e26",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some useful functions:\n",
    "\n",
    "class Voc:\n",
    "    def __init__(self):\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = [\"PAD\", \"UNK\"] # might be changed\n",
    "        self.n_words = 10000 + 2 # might be changed\n",
    "\n",
    "    def unicodeToAscii(self, s):\n",
    "        return ''.join(\n",
    "            c for c in unicodedata.normalize('NFD', s)\n",
    "            if unicodedata.category(c) != 'Mn'\n",
    "        )\n",
    "    def remove_punctuation(self, sentence):\n",
    "        sentence = self.unicodeToAscii(sentence)\n",
    "        sentence = re.sub(r\"([.!?])\", r\" \\1\", sentence)\n",
    "        sentence = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", sentence)\n",
    "        sentence = re.sub(r\"\\s+\", r\" \", sentence).strip()\n",
    "        return sentence\n",
    "\n",
    "    def fit(self, train_df, train_df_no_label, USE_Word2Vec=True):\n",
    "        print(\"Voc fitting...\")\n",
    "        \n",
    "        # tokenize\n",
    "        tokens = []\n",
    "        sentences = []\n",
    "        \n",
    "        for sequence in train_df[\"seq\"]:\n",
    "            token = sequence.strip(\" \").split(\" \")\n",
    "            tokens += token\n",
    "            sentences.append(token)\n",
    "\n",
    "\n",
    "        for sequence in train_df_no_label[\"seq\"]:\n",
    "            token = sequence.strip(\" \").split(\" \")\n",
    "            tokens += token\n",
    "            sentences.append(token)\n",
    "\n",
    "        # Using Word2Vec\n",
    "        if USE_Word2Vec:\n",
    "            dim = 100\n",
    "            print(\"Word2Vec fitting\")\n",
    "            model = Word2Vec(sentences, size=dim, window=5, min_count=20, workers=20, iter=20)\n",
    "            print(\"Word2Vec fitting finished....\")\n",
    "            # gensim index2word \n",
    "            self.index2word += model.wv.index2word\n",
    "            self.n_words = len(self.index2word)\n",
    "            # build up numpy embedding matrix\n",
    "            embedding_matrix = [None] * len(self.index2word) # init to vocab length\n",
    "            embedding_matrix[0] = np.random.normal(size=(dim,))\n",
    "            embedding_matrix[1] = np.random.normal(size=(dim,))\n",
    "            # plug in embedding\n",
    "            for i in range(2, len(self.index2word)):\n",
    "                embedding_matrix[i] = model.wv[self.index2word[i]]\n",
    "                self.word2index[self.index2word[i]] = i\n",
    "            \n",
    "            # \n",
    "            self.embedding_matrix = np.array(embedding_matrix)\n",
    "            return\n",
    "        else:\n",
    "            # Counter\n",
    "            counter = Counter(tokens)\n",
    "            voc_list = counter.most_common(10000)\n",
    "\n",
    "            for i, (voc, freq) in enumerate(voc_list):\n",
    "                self.word2index[voc] = i+2\n",
    "                self.index2word[i+2] = voc\n",
    "                self.word2count[voc] = freq\n",
    "\n",
    "def print_to_csv(y_, filename):\n",
    "    d = {\"id\":[i for i in range(len(y_))],\"label\":list(map(lambda x: str(x), y_))}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "\n",
    "class BOW():\n",
    "    def __init__(self):\n",
    "        self.vectorizer = CountVectorizer(max_features=10000)\n",
    "\n",
    "    def unicodeToAscii(self, s):\n",
    "        return ''.join(\n",
    "            c for c in unicodedata.normalize('NFD', s)\n",
    "            if unicodedata.category(c) != 'Mn'\n",
    "        )\n",
    "    def remove_punctuation(self, sentence):\n",
    "        sentence = self.unicodeToAscii(sentence)\n",
    "        sentence = re.sub(r\"([.!?])\", r\" \\1\", sentence)\n",
    "        sentence = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", sentence)\n",
    "        sentence = re.sub(r\"\\s+\", r\" \", sentence).strip()\n",
    "        return sentence\n",
    "\n",
    "    def fit(self, train_df, train_df_no_label):\n",
    "        # prepare copus\n",
    "    \n",
    "        corpus = list(map(lambda x: self.remove_punctuation(x), train_df['seq']))\n",
    "        corpus += list(map(lambda x: self.remove_punctuation(x), train_df_no_label['seq']))\n",
    "        print(\"BOW fitting\")\n",
    "        self.vectorizer.fit(corpus)\n",
    "        self.dim = len(self.vectorizer.get_feature_names())\n",
    "        print(\"BOW fitting done\")\n",
    "        return self\n",
    "\n",
    "    def batch_generator(self, df, batch_size, shuffle=True, training=True):\n",
    "         # (B, Dimension)\n",
    "        N = df.shape[0]\n",
    "        df_matrix = df.as_matrix()\n",
    "        \n",
    "        if shuffle == True:\n",
    "            random_permutation = np.random.permutation(N)\n",
    "            \n",
    "            # shuffle\n",
    "            X = df_matrix[random_permutation, 1]\n",
    "            y = df_matrix[random_permutation, 0].astype(int) # 0 is label's index\n",
    "        else:\n",
    "            X = df_matrix[:, 1]\n",
    "            y = df_matrix[:, 0].astype(int)\n",
    "        #\n",
    "        quotient = X.shape[0] // batch_size\n",
    "        remainder = X.shape[0] - batch_size * quotient\n",
    "\n",
    "        for i in range(quotient):\n",
    "            batch = {}\n",
    "            batch_X = self.vectorizer.transform(X[i*batch_size:(i+1)*batch_size]).toarray()\n",
    "            batch['X'] = Variable(torch.from_numpy(batch_X)).float()\n",
    "            if training:\n",
    "                batch_y = y[i*batch_size:(i+1)*batch_size]\n",
    "                batch['y'] = Variable(torch.from_numpy(batch_y))\n",
    "            else:\n",
    "                batch['y'] = None\n",
    "            batch['lengths'] = None\n",
    "            yield batch\n",
    "            \n",
    "        if remainder > 0: \n",
    "            batch = {}\n",
    "            batch_X = self.vectorizer.transform(X[-remainder:]).toarray()\n",
    "            batch['X'] = Variable(torch.from_numpy(batch_X)).float()\n",
    "            if training:\n",
    "                batch_y = y[-remainder:]\n",
    "                batch['y'] = Variable(torch.from_numpy(batch_y))\n",
    "            else:\n",
    "                batch['y'] = None\n",
    "            batch['lengths'] = None\n",
    "            yield batch\n",
    "\n",
    "class Preprocess:\n",
    "    '''\n",
    "        Preprocess raw data\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.regex_remove_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "        pass\n",
    "    def unicodeToAscii(self, s):\n",
    "        return ''.join(\n",
    "            c for c in unicodedata.normalize('NFD', s)\n",
    "            if unicodedata.category(c) != 'Mn'\n",
    "        )\n",
    "\n",
    "    def normalizeString(self, sentence):\n",
    "        sentence = self.unicodeToAscii(sentence.strip())\n",
    "        #sentence = self.unicodeToAscii(sentence.lower().strip())\n",
    "        # remove punctuation\n",
    "        if False:\n",
    "            sentence = self.regex_remove_punc.sub('', sentence)\n",
    "        sentence = re.sub(r\"([.!?])\", r\" \\1\", sentence)\n",
    "        sentence = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", sentence)\n",
    "        sentence = re.sub(r\"\\s+\", r\" \", sentence).strip()\n",
    "        return sentence\n",
    "\n",
    "    def remove_punctuation(self, sentence):\n",
    "        sentence = self.regex_remove_punc.sub('', sentence)\n",
    "        return sentence\n",
    "\n",
    "    def read_txt(self, train_filename, test_filename, train_filename_no_label):\n",
    "        train_df = None\n",
    "        test_df = None\n",
    "        train_df_no_label = None\n",
    "        \n",
    "        if train_filename is not None:\n",
    "            train_df = pd.read_csv(train_filename, header=None, names=[\"label\", \"seq\"], sep=\"\\+\\+\\+\\$\\+\\+\\+\",\n",
    "                                  engine=\"python\")\n",
    "            # remove puncuation\n",
    "            #train_df[\"seq\"] = train_df[\"seq\"].apply(lambda seq: self.normalizeString(seq))\n",
    "            \n",
    "        \n",
    "        if test_filename is not None:\n",
    "            with open(test_filename, \"r\") as f:\n",
    "                reader = csv.reader(f, delimiter=\",\")\n",
    "                rows = [[row[0], \",\".join(row[1:])] for row in reader]\n",
    "                test_df = pd.DataFrame(rows[1:], columns=rows[0]) # first row is column name\n",
    "            # remove puncuation\n",
    "            #test_df[\"text\"] = test_df[\"text\"].apply(lambda seq: self.normalizeString(seq))\n",
    "        if train_filename_no_label is not None:\n",
    "            train_df_no_label = pd.read_csv(train_filename_no_label, sep=\"\\n\", header=None, names=[\"seq\"])\n",
    "            train_df_no_label.insert(loc=0, column=\"nan\", value=0)\n",
    "            # remove puncuation\n",
    "            #train_df_no_label[\"seq\"] = train_df_no_label[\"seq\"].apply(lambda seq: self.normalizeString(seq))\n",
    "        \n",
    "        return train_df, test_df, train_df_no_label\n",
    "\n",
    "class Sample_Encode:\n",
    "    '''\n",
    "        Transform \n",
    "    '''\n",
    "    def __init__(self, voc):\n",
    "        self.voc = voc\n",
    "\n",
    "    def sentence_to_index(self, sentence):\n",
    "        encoded = list(map(lambda token: self.voc.word2index[token] if token in self.voc.word2index \\\n",
    "            else UNK_token, sentence))\n",
    "        return encoded\n",
    "\n",
    "    def pad_batch(self, index_batch):\n",
    "        '''\n",
    "            Return padded list with size (B, Max_length)\n",
    "        '''\n",
    "        return list(itertools.zip_longest(*index_batch, fillvalue=PAD_token))\n",
    "\n",
    "    def batch_to_Variable(self, sentence_batch, training=True):\n",
    "        '''\n",
    "            Input: a numpy of sentence\n",
    "            ex. [\"i am a\", \"jim l o \"]\n",
    "\n",
    "            Output: a torch Variable and sentence lengths\n",
    "        '''\n",
    "        # split sentence\n",
    "        sentence_batch = sentence_batch.tolist()\n",
    "        \n",
    "        # apply\n",
    "        for training_sample in sentence_batch:\n",
    "            # split training sentence\n",
    "            training_sample[1] = training_sample[1].strip(\" \").split(\" \")\n",
    "\n",
    "        # encode batch\n",
    "        index_label_batch = [(training_sample[0], self.sentence_to_index(training_sample[1])) \\\n",
    "            for training_sample in sentence_batch]\n",
    "\n",
    "        # sort sentence batch (in order to fit torch pack_pad_sequence)\n",
    "        #index_label_batch.sort(key=lambda x: len(x[1]), reverse=True) \n",
    "        \n",
    "        # index batch\n",
    "        index_batch = [training_sample[1] for training_sample in index_label_batch]\n",
    "        label_batch = [training_sample[0] for training_sample in index_label_batch]\n",
    "\n",
    "        # record batch's length\n",
    "        lengths = [len(indexes) for indexes in index_batch]\n",
    "\n",
    "        # padded batch\n",
    "        padded_batch = self.pad_batch(index_batch)\n",
    "\n",
    "        # transform to Variable\n",
    "        if training:\n",
    "            pad_var = Variable(torch.LongTensor(padded_batch), volatile=False)\n",
    "        else:\n",
    "            pad_var = Variable(torch.LongTensor(padded_batch), volatile=True)\n",
    "\n",
    "        # label\n",
    "        if training:\n",
    "            label_var = Variable(torch.LongTensor(label_batch), volatile=False)\n",
    "        else:\n",
    "            label_var = None\n",
    "\n",
    "        \n",
    "        return pad_var, label_var, lengths\n",
    "    \n",
    "    def generator(self, df, batch_size, shuffle=False, training=True):\n",
    "        '''\n",
    "        Return sample batch Variable\n",
    "            batch['X'] is (T, B)\n",
    "        '''\n",
    "        df_matrix = df.as_matrix()\n",
    "        if shuffle == True:\n",
    "            random_permutation = np.random.permutation(len(df['seq']))\n",
    "            \n",
    "            # shuffle\n",
    "            df_matrix = df_matrix[random_permutation]\n",
    "        #\n",
    "        quotient = df.shape[0] // batch_size\n",
    "        remainder = df.shape[0] - batch_size * quotient\n",
    "\n",
    "        for i in range(quotient):\n",
    "            batch = {}\n",
    "            X, y, lengths = self.batch_to_Variable(df_matrix[i*batch_size:(i+1)*batch_size], training)\n",
    "            batch['X'] = X\n",
    "            batch['y'] = y\n",
    "            batch['lengths'] = lengths\n",
    "            yield batch\n",
    "            \n",
    "        if remainder > 0: \n",
    "            batch = {}\n",
    "            X, y, lengths = self.batch_to_Variable(df_matrix[-remainder:],training)\n",
    "            batch['X'] = X\n",
    "            batch['y'] = y\n",
    "            batch['lengths'] = lengths\n",
    "            yield batch\n",
    "\n",
    "def trim(text_list, threshold=2):\n",
    "    result = []\n",
    "    for _, text in enumerate(text_list):\n",
    "        grouping = []\n",
    "        for _, g in itertools.groupby(text):\n",
    "            grouping.append(list(g))\n",
    "        r = ''.join([g[0] if len(g)<threshold else g[0]*threshold for g in grouping])\n",
    "        result.append(r)\n",
    "    return result\n",
    "\n",
    "def token_counter(corpus):\n",
    "    tokenizer = Tokenizer(num_words=None,filters=\"\\n\")\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    sequences = tokenizer.texts_to_sequences(corpus)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    " \n",
    "stemmer = gensim.parsing.porter.PorterStemmer()\n",
    "def preprocess(string, use_stem = True):\n",
    "    string = string.replace(\"i ' m\", \"im\").replace(\"you ' re\",\"youre\").replace(\"didn ' t\",\"didnt\")    .replace(\"can ' t\",\"cant\").replace(\"haven ' t\", \"havent\").replace(\"won ' t\", \"wont\")    .replace(\"isn ' t\",\"isnt\").replace(\"don ' t\", \"dont\").replace(\"doesn ' t\", \"doesnt\")    .replace(\"aren ' t\", \"arent\").replace(\"weren ' t\", \"werent\").replace(\"wouldn ' t\",\"wouldnt\")    .replace(\"ain ' t\",\"aint\").replace(\"shouldn ' t\",\"shouldnt\").replace(\"wasn ' t\",\"wasnt\")    .replace(\" ' s\",\"s\").replace(\"wudn ' t\",\"wouldnt\").replace(\" .. \",\" ... \")    .replace(\"couldn ' t\",\"couldnt\")\n",
    "    for same_char in re.findall(r'((\\w)\\2{2,})', string):\n",
    "        string = string.replace(same_char[0], same_char[1])\n",
    "    for digit in re.findall(r'\\d+', string):\n",
    "        string = string.replace(digit, \"1\")\n",
    "    for punct in re.findall(r'([-/\\\\\\\\()!\"+,&?\\'.]{2,})',string):\n",
    "        if punct[0:2] ==\"..\":\n",
    "            string = string.replace(punct, \"...\")\n",
    "        else:\n",
    "            string = string.replace(punct, punct[0])\n",
    "    return string\n",
    "\n",
    "def getFrequencyDict(lines):\n",
    "    freq = {}\n",
    "    for s in lines:\n",
    "        for w in s:\n",
    "            if w in freq: freq[w] += 1\n",
    "            else:         freq[w] = 1\n",
    "    return freq\n",
    "\n",
    "def initializeCmap(lines):\n",
    "    print('  Initializing conversion map...')\n",
    "    cmap = {}\n",
    "    for i, s in enumerate(lines):\n",
    "        for j, w in enumerate(s):\n",
    "            cmap[w] = w\n",
    "    print('    Conversion map size:', len(cmap))\n",
    "    return cmap\n",
    "\n",
    "def convertAccents(lines, cmap):\n",
    "    print('  Converting accents...')\n",
    "    for i, s in enumerate(lines):\n",
    "        s = [(''.join(c for c in udata.normalize('NFD', w) if udata.category(c) != 'Mn')) for w in s]\n",
    "        for j, w in enumerate(s):\n",
    "            cmap[original_lines[i][j]] = s[j]\n",
    "        lines[i] = s\n",
    "    clist = 'abcdefghijklmnopqrstuvwxyz0123456789.!?'\n",
    "    for i, s in enumerate(lines):\n",
    "        s = [''.join([c for c in w if c in clist]) for w in s]\n",
    "        for j, w in enumerate(s):\n",
    "            cmap[original_lines[i][j]] = s[j]\n",
    "        lines[i] = s\n",
    "\n",
    "def convertPunctuations(lines, cmap):\n",
    "    print('  Converting punctuations...')\n",
    "    for i, s in enumerate(lines):\n",
    "        for j, w in enumerate(s):\n",
    "            if w == '': continue\n",
    "            excCnt, queCnt, dotCnt = w.count('!'), w.count('?'), w.count('.')\n",
    "            if queCnt:        s[j] = '_?'\n",
    "            elif excCnt >= 5: s[j] = '_!!!'\n",
    "            elif excCnt >= 3: s[j] = '_!!'\n",
    "            elif excCnt >= 1: s[j] = '_!'\n",
    "            elif dotCnt >= 2: s[j] = '_...'\n",
    "            elif dotCnt >= 1: s[j] = '_.'\n",
    "            cmap[original_lines[i][j]] = s[j]\n",
    "        lines[i] = s\n",
    "\n",
    "def convertNotWords(lines, cmap):\n",
    "    print('  Converting not words...')\n",
    "    for i, s in enumerate(lines):\n",
    "        for j, w in enumerate(s):\n",
    "            if w == '': continue\n",
    "            if w[0] == '_': continue\n",
    "            if w == '2':        s[j] = 'to'\n",
    "            elif w.isnumeric(): s[j] = '_n'\n",
    "            cmap[original_lines[i][j]] = s[j]\n",
    "        lines[i] = s\n",
    "\n",
    "def convertTailDuplicates(lines, cmap):\n",
    "    print('  Converting tail duplicates...')\n",
    "    freq = getFrequencyDict(lines)\n",
    "    for i, s in enumerate(lines):\n",
    "        for j, w in enumerate(s):\n",
    "            if w == '': continue\n",
    "            w = re.sub(r'(([a-z])\\2{2,})$', r'\\g<2>\\g<2>', w)\n",
    "            s[j] = re.sub(r'(([a-cg-kmnp-ru-z])\\2+)$', r'\\g<2>', w)\n",
    "            cmap[original_lines[i][j]] = s[j]\n",
    "        lines[i] = s\n",
    "\n",
    "def convertHeadDuplicates(lines, cmap):\n",
    "    print('  Converting head duplicates...')\n",
    "    freq = getFrequencyDict(lines)\n",
    "    for i, s in enumerate(lines):\n",
    "        for j, w in enumerate(s):\n",
    "            if w == '': continue\n",
    "            s[j] = re.sub(r'^(([a-km-z])\\2+)', r'\\g<2>', w)\n",
    "            cmap[original_lines[i][j]] = s[j]\n",
    "        lines[i] = s\n",
    "\n",
    "def convertInlineDuplicates(lines, cmap, minfreq=64):\n",
    "    print('  Converting inline duplicates...')\n",
    "    for i, s in enumerate(lines):\n",
    "        for j, w in enumerate(s):\n",
    "            if w == '': continue\n",
    "            w = re.sub(r'(([a-z])\\2{2,})', r'\\g<2>\\g<2>', w)\n",
    "            s[j] = re.sub(r'(([ahjkquvwxyz])\\2+)', r'\\g<2>', w)  # repeated 2+ times, impossible\n",
    "            cmap[original_lines[i][j]] = s[j]\n",
    "        lines[i] = s\n",
    "    freq = getFrequencyDict(lines)\n",
    "    for i, s in enumerate(lines):\n",
    "        for j, w in enumerate(s):\n",
    "            if w == '': continue\n",
    "            if freq[w] > minfreq: continue\n",
    "            if w == 'too': continue\n",
    "            w1 = re.sub(r'(([a-z])\\2+)', r'\\g<2>', w) # repeated 2+ times, replace by 1\n",
    "            f0, f1 = freq.get(w,0), freq.get(w1,0)\n",
    "            fm = max(f0, f1)\n",
    "            if fm == f0:   pass\n",
    "            else:          s[j] = w1;\n",
    "            cmap[original_lines[i][j]] = s[j]\n",
    "        lines[i] = s\n",
    "\n",
    "def convertSlang(lines, cmap):\n",
    "    print('  Converting slang...')\n",
    "    freq = getFrequencyDict(lines)\n",
    "    for i, s in enumerate(lines):\n",
    "        for j, w in enumerate(s):\n",
    "            if w == '': continue\n",
    "            if w == 'u': lines[i][j] = 'you'\n",
    "            if w == 'dis': lines[i][j] = 'this'\n",
    "            if w == 'dat': lines[i][j] = 'that'\n",
    "            if w == 'luv': lines[i][j] = 'love'\n",
    "            w1 = re.sub(r'in$', r'ing', w)\n",
    "            w2 = re.sub(r'n$', r'ing', w)\n",
    "            f0, f1, f2 = freq.get(w,0), freq.get(w1,0), freq.get(w2,0)\n",
    "            fm = max(f0, f1, f2)\n",
    "            if fm == f0:   pass\n",
    "            elif fm == f1: s[j] = w1;\n",
    "            else:          s[j] = w2;\n",
    "            cmap[original_lines[i][j]] = s[j]\n",
    "        lines[i] = s\n",
    "\n",
    "def convertSingular(lines, cmap, minfreq=512):\n",
    "    print('  Converting singular form...')\n",
    "    freq = getFrequencyDict(lines)\n",
    "    for i, s in enumerate(lines):\n",
    "        for j, w in enumerate(s):\n",
    "            if w == '': continue\n",
    "            if freq[w] > minfreq: continue\n",
    "            w1 = re.sub(r's$', r'', w)\n",
    "            w2 = re.sub(r'es$', r'', w)\n",
    "            w3 = re.sub(r'ies$', r'y', w)\n",
    "            f0, f1, f2, f3 = freq.get(w,0), freq.get(w1,0), freq.get(w2,0), freq.get(w3,0)\n",
    "            fm = max(f0, f1, f2, f3)\n",
    "            if fm == f0:   pass\n",
    "            elif fm == f1: s[j] = w1;\n",
    "            elif fm == f2: s[j] = w2;\n",
    "            else:          s[j] = w3;\n",
    "            cmap[original_lines[i][j]] = s[j]\n",
    "    lines[i] = s\n",
    "\n",
    "def convertRareWords(lines, cmap, min_count=16):\n",
    "    print('  Converting rare words...')\n",
    "    freq = getFrequencyDict(lines)\n",
    "    for i, s in enumerate(lines):\n",
    "        for j, w in enumerate(s):\n",
    "            if w == '': continue\n",
    "            if freq[w] < min_count: s[j] = '_r'\n",
    "            cmap[original_lines[i][j]] = s[j]\n",
    "        lines[i] = s\n",
    "\n",
    "def convertCommonWords(lines, cmap):\n",
    "    print('  Converting common words...')\n",
    "    #beverbs = set('is was are were am s'.split())\n",
    "    #articles = set('a an the'.split())\n",
    "    #preps = set('to for of in at on by'.split())\n",
    "\n",
    "    for i, s in enumerate(lines):\n",
    "        #s = [word if word not in beverbs else '_b' for word in s]\n",
    "        #s = [word if word not in articles else '_a' for word in s]\n",
    "        #s = [word if word not in preps else '_p' for word in s]\n",
    "        lines[i] = s\n",
    "\n",
    "def convertPadding(lines, maxlen=38):\n",
    "    print('  Padding...')\n",
    "    for i, s in enumerate(lines):\n",
    "        lines[i] = [w for w in s if w]\n",
    "    for i, s in enumerate(lines):\n",
    "        lines[i] = s[:maxlen]\n",
    "\n",
    "def preprocessLines(lines):\n",
    "    global original_lines\n",
    "    original_lines = lines[:]\n",
    "    cmap = initializeCmap(original_lines)\n",
    "    convertAccents(lines, cmap)\n",
    "    convertPunctuations(lines, cmap)\n",
    "    convertNotWords(lines, cmap)\n",
    "    convertTailDuplicates(lines, cmap)\n",
    "    convertHeadDuplicates(lines, cmap)\n",
    "    convertInlineDuplicates(lines, cmap)\n",
    "    convertSlang(lines, cmap)\n",
    "    convertSingular(lines, cmap)\n",
    "    convertRareWords(lines, cmap)\n",
    "    convertCommonWords(lines, cmap)\n",
    "    convertPadding(lines)\n",
    "    return lines, cmap\n",
    "\n",
    "def readData(path, label=True):\n",
    "    print('  Loading', path+'...')\n",
    "    _lines, _labels = [], []\n",
    "    with open(path, 'r', encoding='utf_8') as f:\n",
    "        for line in f:\n",
    "            if label:\n",
    "                _labels.append(int(line[0]))\n",
    "                line = line[10:-1]\n",
    "            else:\n",
    "                line = line[:-1]\n",
    "            _lines.append(line.split())\n",
    "    if label: return _lines, _labels\n",
    "    else:     return _lines\n",
    "\n",
    "def padLines(lines, value, maxlen):\n",
    "    maxlinelen = 0\n",
    "    for i, s in enumerate(lines):\n",
    "        maxlinelen = max(len(s), maxlinelen)\n",
    "    maxlinelen = max(maxlinelen, maxlen)\n",
    "    for i, s in enumerate(lines):\n",
    "        lines[i] = (['_r'] * max(0, maxlinelen - len(s)) + s)[-maxlen:]\n",
    "    return lines\n",
    "\n",
    "def getDictionary(lines):\n",
    "    _dict = {}\n",
    "    for s in lines:\n",
    "        for w in s:\n",
    "            if w not in _dict:\n",
    "                _dict[w] = len(_dict) + 1\n",
    "    return _dict\n",
    "\n",
    "def transformByDictionary(lines, dictionary):\n",
    "    for i, s in enumerate(lines):\n",
    "        for j, w in enumerate(s):\n",
    "            if w in dictionary: lines[i][j] = dictionary[w]\n",
    "            else:               lines[i][j] = dictionary['']\n",
    "\n",
    "def transformByConversionMap(lines, cmap, iter=2):\n",
    "    cmapRefine(cmap)\n",
    "    for it in range(iter):\n",
    "        for i, s in enumerate(lines):\n",
    "            s0 = []\n",
    "            for j, w in enumerate(s):\n",
    "                if w in cmap and w[0] != '_':\n",
    "                    s0 = s0 + cmap[w].split()\n",
    "                elif w[0] == '_':\n",
    "                    s0 = s0 + [w]\n",
    "            lines[i] = [w for w in s0 if w]\n",
    "\n",
    "def transformByWord2Vec(lines, w2v):\n",
    "    for i, s in enumerate(lines):\n",
    "        for j, w in enumerate(s):\n",
    "            if w in w2v.wv:\n",
    "                lines[i][j] = w2v.wv[w]\n",
    "            else:\n",
    "                lines[i][j] = w2v.wv['_r']\n",
    "\n",
    "def readTestData(path):\n",
    "    print('  Loading', path + '...')\n",
    "    _lines = []\n",
    "    with open(path, 'r', encoding='utf_8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i:\n",
    "                start = int(np.log10(max(1, i-1))) + 2\n",
    "                _lines.append(line[start:].split())\n",
    "    return _lines\n",
    "\n",
    "def savePrediction(y, path, id_start=0):\n",
    "    pd.DataFrame([[i+id_start, int(y[i])] for i in range(y.shape[0])],\n",
    "                 columns=['id', 'label']).to_csv(path, index=False)\n",
    "\n",
    "def savePreprocessCorpus(lines, path):\n",
    "    with open(path, 'w', encoding='utf_8') as f:\n",
    "        for line in lines:\n",
    "            f.write(' '.join(line) + '\\n')\n",
    "\n",
    "def savePreprocessCmap(cmap, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(cmap, f)\n",
    "\n",
    "def loadPreprocessCmap(path):\n",
    "    print('  Loading', path + '...')\n",
    "    with open(path, 'rb') as f:\n",
    "        cmap = pickle.load(f)\n",
    "    return cmap\n",
    "\n",
    "def loadPreprocessCorpus(path):\n",
    "    print('  Loading', path + '...')\n",
    "    lines = []\n",
    "    with open(path, 'r', encoding='utf_8') as f:\n",
    "        for line in f:\n",
    "            lines.append(line.split())\n",
    "    return lines\n",
    "\n",
    "def removePunctuations(lines):\n",
    "    rs = {'_!', '_!!', '_!!!', '_.', '_...', '_?'}\n",
    "    for i, s in enumerate(lines):\n",
    "        for j, w in enumerate(s):\n",
    "            if w in rs:\n",
    "                s[j] = ''\n",
    "        lines[i] = [w for w in x if w]\n",
    "\n",
    "def removeDuplicatedLines(lines):\n",
    "    lineset = set({})\n",
    "    for line in lines:\n",
    "        lineset.add(' '.join(line))\n",
    "    for i, line in enumerate(lineset):\n",
    "        lines[i] = line.split()\n",
    "    del lines[-(len(lines)-len(lineset)):]\n",
    "    return lineset\n",
    "\n",
    "def shuffleData(lines, labels):\n",
    "    for i, s in enumerate(lines):\n",
    "        lines[i] = (s, labels[i])\n",
    "    np.random.shuffle(lines)\n",
    "    for i, s in enumerate(lines):\n",
    "        labels[i] = s[1]\n",
    "        lines[i] = s[0]\n",
    "\n",
    "def cmapRefine(cmap):\n",
    "    cmap['teh'] = cmap['da'] = cmap['tha'] = 'the'\n",
    "    cmap['evar'] = 'ever'\n",
    "    cmap['likes'] = cmap['liked'] = cmap['lk'] = 'like'\n",
    "    cmap['wierd'] = 'weird'\n",
    "    cmap['kool'] = 'cool'\n",
    "    cmap['yess'] = 'yes'\n",
    "    cmap['pleasee'] = 'please'\n",
    "    cmap['soo'] = 'so'\n",
    "    cmap['noo'] = 'no'\n",
    "    cmap['lovee'] = cmap['loove'] = cmap['looove'] = cmap['loooove'] = cmap['looooove'] \\\n",
    "        = cmap['loooooove'] = cmap['loves'] = cmap['loved'] = cmap['wuv'] \\\n",
    "        = cmap['loovee'] = cmap['lurve'] = cmap['lov'] = cmap['luvs'] = 'love'\n",
    "    cmap['lovelove'] = 'love love'\n",
    "    cmap['lovelovelove'] = 'love love love'\n",
    "    cmap['ilove'] = 'i love'\n",
    "    cmap['liek'] = cmap['lyk'] = cmap['lik'] = cmap['lke'] = cmap['likee'] = 'like'\n",
    "    cmap['mee'] = 'me'\n",
    "    cmap['hooo'] = 'hoo'\n",
    "    cmap['sooon'] = cmap['soooon'] = 'soon'\n",
    "    cmap['goodd'] = cmap['gud'] = 'good'\n",
    "    cmap['bedd'] = 'bed'\n",
    "    cmap['badd'] = 'bad'\n",
    "    cmap['sadd'] = 'sad'\n",
    "    cmap['madd'] = 'mad'\n",
    "    cmap['redd'] = 'red'\n",
    "    cmap['tiredd'] = 'tired'\n",
    "    cmap['boredd'] = 'bored'\n",
    "    cmap['godd'] = 'god'\n",
    "    cmap['xdd'] = 'xd'\n",
    "    cmap['itt'] = 'it'\n",
    "    cmap['lul'] = cmap['lool'] = 'lol'\n",
    "    cmap['sista'] = 'sister'\n",
    "    cmap['w00t'] = 'woot'\n",
    "    cmap['srsly'] = 'seriously'\n",
    "    cmap['4ever'] = cmap['4eva'] = 'forever'\n",
    "    cmap['neva'] = 'never'\n",
    "    cmap['2day'] = 'today'\n",
    "    cmap['homee'] = 'home'\n",
    "    cmap['hatee'] = 'hate'\n",
    "    cmap['heree'] = 'here'\n",
    "    cmap['cutee'] = 'cute'\n",
    "    cmap['lemme'] = 'let me'\n",
    "    cmap['mrng'] = 'morning'\n",
    "    cmap['gd'] = 'good'\n",
    "    cmap['thx'] = cmap['thnx'] = cmap['thanx'] = cmap['thankx'] = cmap['thnk'] = 'thanks'\n",
    "    cmap['jaja'] = cmap['jajaja'] = cmap['jajajaja'] = 'haha'\n",
    "    cmap['eff'] = cmap['fk'] = cmap['fuk'] = cmap['fuc'] = 'fuck'\n",
    "    cmap['2moro'] = cmap['2mrow'] = cmap['2morow'] = cmap['2morrow'] \\\n",
    "        = cmap['2morro'] = cmap['2mrw'] = cmap['2moz'] = 'tomorrow'\n",
    "    cmap['babee'] = 'babe'\n",
    "    cmap['theree'] = 'there'\n",
    "    cmap['thee'] = 'the'\n",
    "    cmap['woho'] = cmap['wohoo'] = 'woo hoo'\n",
    "    cmap['2gether'] = 'together'\n",
    "    cmap['2nite'] = cmap['2night'] = 'tonight'\n",
    "    cmap['nite'] = 'night'\n",
    "    cmap['dnt'] = 'dont'\n",
    "    cmap['rly'] = 'really'\n",
    "    cmap['gt'] = 'get'\n",
    "    cmap['lat'] = 'late'\n",
    "    cmap['dam'] = 'damn'\n",
    "    cmap['4ward'] = 'forward'\n",
    "    cmap['4give'] = 'forgive'\n",
    "    cmap['b4'] = 'before'\n",
    "    cmap['tho'] = 'though'\n",
    "    cmap['kno'] = 'know'\n",
    "    cmap['grl'] = 'girl'\n",
    "    cmap['boi'] = 'boy'\n",
    "    cmap['wrk'] = 'work'\n",
    "    cmap['jst'] = 'just'\n",
    "    cmap['geting'] = 'getting'\n",
    "    cmap['4get'] = 'forget'\n",
    "    cmap['4got'] = 'forgot'\n",
    "    cmap['4real'] = 'for real'\n",
    "    cmap['2go'] = 'to go'\n",
    "    cmap['2b'] = 'to be'\n",
    "    cmap['gr8'] = cmap['gr8t'] = cmap['gr88'] = 'great'\n",
    "    cmap['str8'] = 'straight'\n",
    "    cmap['twiter'] = 'twitter'\n",
    "    cmap['iloveyou'] = 'i love you'\n",
    "    cmap['loveyou'] = cmap['loveya'] = cmap['loveu'] = 'love you'\n",
    "    cmap['xoxox'] = cmap['xox'] = cmap['xoxoxo'] = cmap['xoxoxox'] \\\n",
    "        = cmap['xoxoxoxo'] = cmap['xoxoxoxoxo'] = 'xoxo'\n",
    "    cmap['cuz'] = cmap['bcuz'] = cmap['becuz'] = 'because'\n",
    "    cmap['iz'] = 'is'\n",
    "    cmap['aint'] = 'am not'\n",
    "    cmap['fav'] = 'favorite'\n",
    "    cmap['ppl'] = 'people'\n",
    "    cmap['mah'] = 'my'\n",
    "    cmap['r8'] = 'rate'\n",
    "    cmap['l8'] = 'late'\n",
    "    cmap['w8'] = 'wait'\n",
    "    cmap['m8'] = 'mate'\n",
    "    cmap['h8'] = 'hate'\n",
    "    cmap['l8ter'] = cmap['l8tr'] = cmap['l8r'] = 'later'\n",
    "    cmap['cnt'] = 'cant'\n",
    "    cmap['fone'] = cmap['phonee'] = 'phone'\n",
    "    cmap['f1'] = 'fONE'\n",
    "    cmap['xboxe3'] = 'eTHREE'\n",
    "    cmap['jammin'] = 'jamming'\n",
    "    cmap['onee'] = 'one'\n",
    "    cmap['1st'] = 'first'\n",
    "    cmap['2nd'] = 'second'\n",
    "    cmap['3rd'] = 'third'\n",
    "    cmap['inet'] = 'internet'\n",
    "    cmap['recomend'] = 'recommend'\n",
    "    cmap['ah1n1'] = cmap['h1n1'] = 'hONEnONE'\n",
    "    cmap['any1'] = 'anyone'\n",
    "    cmap['every1'] = cmap['evry1'] = 'everyone'\n",
    "    cmap['some1'] = cmap['sum1'] = 'someone'\n",
    "    cmap['no1'] = 'no one'\n",
    "    cmap['4u'] = 'for you'\n",
    "    cmap['4me'] = 'for me'\n",
    "    cmap['2u'] = 'to you'\n",
    "    cmap['yu'] = 'you'\n",
    "    cmap['yr'] = cmap['yrs'] = cmap['years'] = 'year'\n",
    "    cmap['hr'] = cmap['hrs'] = cmap['hours'] = 'hour'\n",
    "    cmap['min'] = cmap['mins'] = cmap['minutes'] = 'minute'\n",
    "    cmap['go2'] = cmap['goto'] = 'go to'\n",
    "    for key, value in cmap.items():\n",
    "        if not key.isalpha():\n",
    "            if key[-1:] == 'k':\n",
    "                cmap[key] = '_n'\n",
    "            if key[-2:]=='st' or key[-2:]=='nd' or key[-2:]=='rd' or key[-2:]=='th':\n",
    "                cmap[key] = '_ord'\n",
    "            if key[-2:]=='am' or key[-2:]=='pm' or key[-3:]=='min' or key[-4:]=='mins' \\\n",
    "                    or key[-2:]=='hr' or key[-3:]=='hrs' or key[-1:]=='h' \\\n",
    "                    or key[-4:]=='hour' or key[-5:]=='hours'\\\n",
    "                    or key[-2:]=='yr' or key[-3:]=='yrs'\\\n",
    "                    or key[-3:]=='day' or key[-4:]=='days'\\\n",
    "                    or key[-3:]=='wks':\n",
    "                cmap[key] = '_time'\n",
    "def preprocessTestingData(path):\n",
    "    print('Loading testing data...')\n",
    "    lines = readTestData(path)\n",
    "\n",
    "    cmap_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'model/cmap.pkl')\n",
    "    w2v_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'model/word2vec.pkl')\n",
    "    cmap = loadPreprocessCmap(cmap_path)\n",
    "    transformByConversionMap(lines, cmap)\n",
    "    \n",
    "    lines = padLines(lines, '_', maxlen)\n",
    "    w2v = Word2Vec.load(w2v_path)\n",
    "    transformByWord2Vec(lines, w2v)\n",
    "    return lines\n",
    "\n",
    "def preprocessTrainingData(label_path, nolabel_path, retrain=False, punctuation=True):\n",
    "    print('Loading training data...')\n",
    "    if retrain:\n",
    "        preprocess(label_path, nolabel_path)\n",
    "\n",
    "    lines, labels = readData(label_path)\n",
    "    corpus_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'model/corpus.txt')\n",
    "    cmap_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'model/cmap.pkl')\n",
    "    w2v_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'model/word2vec.pkl')\n",
    "    lines = readData(corpus_path, label=False)[:len(lines)]\n",
    "    shuffleData(lines, labels)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    cmap = loadPreprocessCmap(cmap_path)\n",
    "    transformByConversionMap(lines, cmap)\n",
    "    if not punctuation:\n",
    "        removePunctuations(lines)\n",
    "\n",
    "    lines = padLines(lines, '_', maxlen)\n",
    "    w2v = Word2Vec.load(w2v_path)\n",
    "    transformByWord2Vec(lines, w2v)\n",
    "    return lines, labels\n",
    "\n",
    "def preprocess(label_path, nolabel_path):\n",
    "    print('Preprocessing...')\n",
    "    labeled_lines, labels = readData(label_path)\n",
    "    nolabel_lines = readData(nolabel_path, label=False)\n",
    "    lines = labeled_lines + nolabel_lines\n",
    "\n",
    "    lines, cmap = preprocessLines(lines)\n",
    "    corpus_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'model/corpus.txt')\n",
    "    cmap_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'model/cmap.pkl')\n",
    "    w2v_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'model/word2vec.pkl')\n",
    "    savePreprocessCorpus(lines, corpus_path)\n",
    "    savePreprocessCmap(cmap, cmap_path)\n",
    "\n",
    "    transformByConversionMap(lines, cmap)\n",
    "    removeDuplicatedLines(lines)\n",
    "\n",
    "    print('Training word2vec...')\n",
    "    model = Word2Vec(lines, size=256, min_count=16, iter=16, workers=16)\n",
    "    model.save(w2v_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "3bbdfbc37cfda81512ee930a172c819cf7a442e9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# search from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "f615b130f977365f1a1c33281768350d86cff49a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.columns = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"text\"] # give column names\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "ec401a17982fe809aad9e3cba1460d6a2b39eec9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop([\"id\", \"date\", \"query\", \"user\"], axis = 1) #drop some column from the dataframe \n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "6a4e2d515aa7198e6beeb07512069f4a6aed13e0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.head() # get the first 5 rows from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "1250dedbdeeea451a6a77a2f7d8392e1bd43babd",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.sentiment.value_counts() # count the number of sentiments with respect to their tweet(4 stands for positive tweet and 0 stands for negative tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "2f599159b02af198749ecb2d62ded906cd482b2b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['pre_clean_len'] = [len(t) for t in df.text] # add new column pre_clean_len to dataframe which is length of each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "6ea295308a420efc19b769314bafdf3198cc9a85",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.boxplot(df.pre_clean_len) # plot pre_clean_len column\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "519d0dcb51b65edfbc5a2c773e3cb3cbd1acab1c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[df.pre_clean_len > 140].head(10)  # check for any tweets greater than 140 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "9641f22e5ab93b958487d7516c6b25ae6d096d7f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tok = WordPunctTokenizer()\n",
    "\n",
    "pat1 = r'@[A-Za-z0-9_]+'        # remove @ mentions fron tweets\n",
    "pat2 = r'https?://[^ ]+'        # remove URL's from tweets\n",
    "combined_pat = r'|'.join((pat1, pat2)) #addition of pat1 and pat2\n",
    "www_pat = r'www.[^ ]+'         # remove URL's from tweets\n",
    "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",   # converting words like isn't to is not\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "\n",
    "def tweet_cleaner(text):  # define tweet_cleaner function to clean the tweets\n",
    "    soup = BeautifulSoup(text, 'lxml')    # call beautiful object\n",
    "    souped = soup.get_text()   # get only text from the tweets \n",
    "    try:\n",
    "        bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")    # remove utf-8-sig codeing\n",
    "    except:\n",
    "        bom_removed = souped\n",
    "    stripped = re.sub(combined_pat, '', bom_removed) # calling combined_pat\n",
    "    stripped = re.sub(www_pat, '', stripped) #remove URL's\n",
    "    lower_case = stripped.lower()      # converting all into lower case\n",
    "    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case) # converting word's like isn't to is not\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)       # will replace # by space\n",
    "    words = [x for x  in tok.tokenize(letters_only) if len(x) > 1] # Word Punct Tokenize and only consider words whose length is greater than 1\n",
    "    return (\" \".join(words)).strip() # join the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "bf0c499e4152e168aa1deda2d59fde0cc4dd7109",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nums = [0,400000,800000,1200000,1600000] # used for batch processing tweets\n",
    "#nums = [0, 9999]\n",
    "clean_tweet_texts = [] # initialize list\n",
    "for i in range(nums[0],nums[4]): # batch process 1.6 million tweets                                                               \n",
    "    clean_tweet_texts.append(tweet_cleaner(df['text'][i]))  # call tweet_cleaner function and pass parameter as all the tweets to clean the tweets and append cleaned tweets into clean_tweet_texts list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "f389cd62a8d96993a80afeddf3fc3fdba7302ac5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clean_tweet_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "a6e7070d8e2735c8ac2e29c44ddcc0bbebef31ae",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_tokens = [] # initialize list for tokens\n",
    "for word in clean_tweet_texts:  # for each word in clean_tweet_texts\n",
    "    word_tokens.append(word_tokenize(word)) #tokenize word in clean_tweet_texts and append it to word_tokens list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "4f4e38a4176f889989cad59b93172855e10daf09",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word_tokens\n",
    "# stop = set(stopwords.words('english'))\n",
    "# clean_df =[]\n",
    "# for m in word_tokens:\n",
    "#     a = [w for w in m if not w in stop]\n",
    "#     clean_df.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "b5f3aa97e461d148cfce46a0c5cdd637004e8515",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "c376d84969410ad01103909baf916f47e8ed7337",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = [] # initialize list df1 to store words after lemmatization\n",
    "from nltk.stem import WordNetLemmatizer # import WordNetLemmatizer from nltk.stem\n",
    "lemmatizer = WordNetLemmatizer() # create an object of WordNetLemmatizer\n",
    "for l in word_tokens: # for loop for every tokens in word_token\n",
    "    b = [lemmatizer.lemmatize(q) for q in l] #for every tokens in word_token lemmatize word and giev it to b\n",
    "    df1.append(b) #append b to list df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "37ba3af39bd34a5ec04ec06814b00b2606721c81",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "d0273909276503f8a61c08095a1c740fc4738603",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df1 = [] \n",
    "# from nltk.stem import PorterStemmer\n",
    "# ps = PorterStemmer()\n",
    "# for l in word_tokens:\n",
    "#     b = [ps.stem(q) for q in l]\n",
    "#     df1.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "286cb16eb47e24236108fc31ed58ab3a99fad6d7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "1dd781d617f0cf1e5f583956700a5c7966e8c5fe",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_df1 =[] # initialize list clean_df1 to join word tokens after lemmatization\n",
    "for c in df1:  # for loop for each list in df1\n",
    "    a = \" \".join(c) # join words in list with space in between and giev it to a\n",
    "    clean_df1.append(a) # append a to clean_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "aece6a5f546a28e6e7515665eec4eb491c48c8f9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clean_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "3fe4acfe06158394247cb5cb3f414c6b6aad9b54",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_df = pd.DataFrame(clean_df1,columns=['text']) # convert clean_tweet_texts into dataframe and name it as clean_df\n",
    "clean_df['target'] = df.sentiment # from earlier dataframe get the sentiments of each tweet and make a new column in clean_df as target and give it all the sentiment score\n",
    "#clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "b531767a36659cbb8ea15790badf59593151045b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_df['clean_len'] = [len(t) for t in clean_df.text] # Again make a new coloumn in the dataframe and name it as clean_len which will store thw number of words in the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "a42b82b0b704d100845b53330b84675c3aa09715",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_df[clean_df.clean_len > 140].head(10) # agin check id any tweet is more than 140 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "502197db9f4715c524aac0bfd2c78e0dae594b11",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = clean_df.text # get all the text in x variable\n",
    "y = clean_df.target # get all the sentiments into y variable\n",
    "print(X.shape) #print shape of x\n",
    "print(y.shape) # print shape of y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "e2e5c8903bbad5eb74485957eb670152990df453",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split #from sklearn.cross_validation import train_test_split to split the data into training and tesing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state= 1) # split the data into traing and testing set where ratio is 80:20\n",
    "\n",
    "\n",
    "# X_train is the tweets of training data, X_test is the testing tweets which we have to predict, y_train is the sentiments of tweets in the traing data and y_test is the sentiments of the tweets  which we will use to measure the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "8071b06cc0876fa60e17c21dc667fda7992e0257",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(analyzer = \"word\", ngram_range=(1,3)) # Get Tf-idf object and save it as vect. We can select features from here we just have simply change \n",
    "                                                                                     #the ngram range to change the features also we can remove stop words over here with the help of stop parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "e7075b6bb0a7642e555a3ddaea76454d733add36",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vect.fit(X_train) # fit or traing data tweets to vect\n",
    "X_train_dtm = vect.transform(X_train) # transform our training data tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "71e8a04a9a11f0cfa9ed6db972dc500455f8cc49",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_dtm = vect.transform(X_test)# transform our testing data tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "5522e26964a963b88c4ea8856e0496cbbf927edc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB # import Multinomial Naive Bayes model from sklearn.naive_bayes\n",
    "nb = MultinomialNB(alpha = 10) # get object of Multinomial naive bayes model with alpha parameter = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "846e0ce94403c243943d48d0cfefc8ecbd6ae656",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb.fit(X_train_dtm, y_train)# fit our both traing data tweets as well as its sentiments to the multinomial naive bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "e323e5e25dc40a41646248354ccb7346d87dabd3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score  # import cross_val_score from sklear.model_selection\n",
    "accuracies = cross_val_score(estimator = nb, X = X_train_dtm, y = y_train, cv = 10) # do K- fold cross validation on our traing data and its sentimenst with 10 fold cross validation\n",
    "accuracies.mean() # measure the mean accuray of 10 fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "03d1edab79763c6767091bed4467f2d6cd03f5b5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_nb = nb.predict(X_test_dtm) # predict the sentiments of testing data tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_uuid": "6121caacf805586aeb555a1287913222c08531dc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics # import metrics from sklearn\n",
    "metrics.accuracy_score(y_test, y_pred_nb) # measure the accuracy of our model on the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_uuid": "e74e7741a785136ace290cf60f44156c085352ca",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix # import confusion matrix from the sklearn.metrics\n",
    "confusion_matrix(y_test, y_pred_nb) # plot the confusion matrix between our predicted sentiments and the original testing data sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_uuid": "785716d610ecf205c87613086325ff195d2b41b5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression # import Logistic Regression model from sklearn.linear_model\n",
    "logisticRegr = LogisticRegression(C = 1.1) # get object of logistic regression model with cost parameter = 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_uuid": "635229691aa44ffd92444a78782ace12f485bf12",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logisticRegr.fit(X_train_dtm, y_train)# fit our both traing data tweets as well as its sentiments to the logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_uuid": "4ab45adb5cc9f940f51056371d647bf2bdcc2c97",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score # import cross_val_score from sklear.model_selection\n",
    "accuracies = cross_val_score(estimator = logisticRegr, X = X_train_dtm, y = y_train, cv = 10) # do K- fold cross validation on our traing data and its sentimenst with 10 fold cross validation\n",
    "accuracies.mean() # measure the mean accuray of 10 fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_uuid": "d070eca7ba81212f5bc5009d5c98ebdfcbec809e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_lg = logisticRegr.predict(X_test_dtm)  # predict the sentiments of testing data tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_uuid": "ff95d066b9acc95e9b2734a6475a5b9b85168fc5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics # import metrics from sklearn\n",
    "metrics.accuracy_score(y_test, y_pred_lg) # measure the accuracy of our model on the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_uuid": "776c92bd3af3c3b5b2da6f36e46143d734cf41b0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix # import confusion matrix from the sklearn.metrics\n",
    "confusion_matrix(y_test, y_pred_lg) # plot the confusion matrix between our predicted sentiments and the original testing data sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_uuid": "77e90f4e916f3a86a5c1ce618690694343d6e5e7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC # import SVC model from sklearn.svm\n",
    "svm_clf = LinearSVC(random_state=0) # get object of SVC model with random_state parameter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_uuid": "0618cb437c54fa8136979a9d583352f5f3ee9083",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm_clf.fit(X_train_dtm, y_train)# fit our both traing data tweets as well as its sentiments to the SVC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_uuid": "e1b0f04196ab17e8589bfe01fa6e1779ba3c7748",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score  # import cross_val_score from sklear.model_selection\n",
    "accuracies = cross_val_score(estimator = svm_clf, X = X_train_dtm, y = y_train, cv = 10)# do K- fold cross validation on our traing data and its sentimenst with 10 fold cross validation\n",
    "accuracies.mean() # measure the mean accuray of 10 fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_uuid": "f4690b37c804a18f70fc3cada9b35f38d0b0eef9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_svm = svm_clf.predict(X_test_dtm)  # predict the sentiments of testing data tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_uuid": "85e37d2b750faee33b964b2bf11de8ed9bd937ed",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics  # import metrics from sklearn\n",
    "metrics.accuracy_score(y_test, y_pred_svm)  # measure the accuracy of our model on the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_uuid": "a3222549cc22d2cde3a5a3f566f98f3041a382d9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix # import confusion matrix from the sklearn.metrics\n",
    "confusion_matrix(y_test, y_pred_svm)# plot the confusion matrix between our predicted sentiments and the original testing data sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0dec928a982861724a2fa497b057953acfae08fb",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
