{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project PD2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import datascience as ds\n",
    "from datascience import *\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from graphviz import Source\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.externals import joblib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tweets data loaded into Jupyter Notebook as Table object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = ds.Table.read_table('Climate1SupportiveLevel.csv', sep=',')\n",
    "df2 = ds.Table.read_table('ClimateBalancedDS2.csv', sep=',')\n",
    "df = df1.append(df2)\n",
    "test_index = np.random.choice(df.num_rows, 100, replace=False)\n",
    "train_val_index = [i for i in np.arange(df.num_rows) if i not in test_index]\n",
    "test_data = df.take[test_index]\n",
    "df = df.take[train_val_index]\n",
    "X = list(df['Text'])\n",
    "y = list(df['Support'])\n",
    "test_X = list(test_data['Text'])\n",
    "test_y = list(test_data['Support'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A description of all model enhancements incorporated into the construction of PD2.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### used parameters\n",
    "\n",
    "- CountVector\n",
    "    - token_pattern = \"(?!RT|rt|\\d+)[@#]*[\\w\\'_-]{2,100}\"\n",
    "    - analyzer = 'word'\n",
    "    - stop_words = the top 30 most common words in DS1+DS2 tweets \n",
    "    - min_df = 10\n",
    "    - ngram_range=(1,2)\n",
    "- DecisionTree\n",
    "    - criterion='entropy'\n",
    "    - max_depth = 10\n",
    "    - min_samples_leaf = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A description of all model parameters you tried and the associated Stratified k-fold cross validation results for each model parameter choice based on the combined data set (DS1 + DS2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - max_depth=range(1, 15)\n",
    " - stop_words:\n",
    "     - default in CountVectorizer: 'english'\n",
    "     - user_defined: ['a', 'an', 'the', 'it', 'is', 'are', 'be', 'of', 'this', 'that', 'RT', 'rt','https']\n",
    "     - the top 30 most common words in DS1+DS2 tweets (chosen)\n",
    "     \n",
    "code and test results as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Check whether the data distribution is balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def check(sentiment, index, note='training'):\n",
    "    if sentiment==0:\n",
    "        label = 'not supportive'\n",
    "    else:\n",
    "        label = 'supportive'\n",
    "    print('There are {} '.format(df.take(index).where('Support', \n",
    "          are.equal_to(sentiment)).size[0][0])+label+' tweets in the '+note+' set.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def custom_split(train_index, test_index):\n",
    "    trainingset = df.take(train_index)\n",
    "    testingset = df.take(test_index)    \n",
    "        \n",
    "    X_train= list(trainingset['Text'])\n",
    "    y_train= list(trainingset['Support'])\n",
    "    X_test= list(testingset['Text'])\n",
    "    y_test= list(testingset['Support'])\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def classifier(X_train, y_train, X_test, fold, max_depth, min_samples_leaf, stop_words, overfit_risk=False):\n",
    "    # token_pattern='(([#@]|[0-9]|[a-z]|[A-Z])+)'\n",
    "    clf = Pipeline(\n",
    "        [\n",
    "            ('vect', CountVectorizer(token_pattern=\"(?!RT|rt|\\d+)[@#]*[\\w\\'_-]{2,100}\",\n",
    "                                     analyzer = 'word',\n",
    "                                     stop_words = stop_words,\n",
    "                                     min_df = 10,\n",
    "                                     ngram_range=(1,2))),\n",
    "            ('clf', DecisionTreeClassifier(criterion='entropy',\n",
    "                                           random_state = 100,\n",
    "                                           max_depth = max_depth,\n",
    "                                           min_samples_leaf = min_samples_leaf))\n",
    "        ])\n",
    "    clf.fit(X_train, y_train)\n",
    "    feature_names = clf.named_steps['vect'].get_feature_names()\n",
    "    try:\n",
    "        dot_data = tree.export_graphviz(clf.named_steps['clf'], out_file=None, \n",
    "                                        feature_names=feature_names)\n",
    "        graph = Source(dot_data)\n",
    "        graph.render('ClimateClassifier-Fold_{}'.format(fold))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    predicted_y_train = clf.predict(X_train)\n",
    "    predicted_y_test = clf.predict(X_test)\n",
    "    # save as pickle\n",
    "    if overfit_risk:\n",
    "        joblib.dump(clf, 'ClimateTeam7PD2_maxdepth{}.pkl'.format(max_depth))\n",
    "    else:\n",
    "        joblib.dump(clf, 'ClimateTeam7PD2.pkl')\n",
    "    return predicted_y_train, predicted_y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def eval_results(predicted_y_train, y_train, predicted_y_test, y_test, fold):\n",
    "    accuracy_s = accuracy_score(y_test, predicted_y_test)\n",
    "    precision_s = precision_score(y_test, predicted_y_test)\n",
    "    recall_s = recall_score(y_test, predicted_y_test)\n",
    "    f1_s = f1_score(y_test, predicted_y_test)\n",
    "    cm_train = confusion_matrix(y_train, predicted_y_train)\n",
    "    cm_test = confusion_matrix(y_test, predicted_y_test)  \n",
    "    \n",
    "    print('Accuracy Score:', accuracy_s)\n",
    "    print(\"Precision Score:\", precision_s)\n",
    "    print(\"Recall Score:\", recall_s)\n",
    "    print(\"f1 Score:\", f1_s)\n",
    "    print('confusion_matrix of training set is: \\n', cm_train, '\\n')\n",
    "    print('confusion_matrix of testing set is: \\n', cm_test, '\\n')\n",
    "    print(classification_report(y_test, predicted_y_test))\n",
    "    \n",
    "    classes = ['not supportive', 'supportive']\n",
    "    plt.subplot(2, 5, fold)\n",
    "    sns.heatmap(cm_train, annot=True, cmap='Blues', yticklabels=classes, xticklabels=classes)\n",
    "    plt.title('Fold {}: confusion matrix of training set'.format(fold))\n",
    "    plt.subplot(2, 5, fold+5)\n",
    "    sns.heatmap(cm_test, annot=True, cmap='Blues', yticklabels=classes, xticklabels=classes)\n",
    "    plt.title('Fold {}: confusion matrix of testing set'.format(fold))\n",
    "    return accuracy_s, precision_s, recall_s, f1_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def k_fold_evaluate(X, y, max_depth, min_samples_leaf, stop_words, print_eval=True, overfit_risk=False):\n",
    "    # initialization\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall=[]\n",
    "    f1 = []\n",
    "    fold = 1\n",
    "    skf = StratifiedKFold(n_splits=5, random_state=1, shuffle= True)\n",
    "    \n",
    "    # build model and collect results\n",
    "    for val_index, test_index in skf.split(X, y):\n",
    "        X_train, X_val, y_train, y_val = custom_split(val_index, test_index)\n",
    "        \n",
    "        predicted_y_train, predicted_y_val = classifier(X_train=X_train, y_train=y_train, \n",
    "                                                          X_test=X_val, fold=fold,\n",
    "                                                          max_depth = max_depth,\n",
    "                                                          min_samples_leaf = min_samples_leaf,\n",
    "                                                          stop_words = stop_words,\n",
    "                                                          overfit_risk=overfit_risk)\n",
    "        metrics_df={}\n",
    "        if print_eval:\n",
    "            print('\\nFold: {}'.format(fold))\n",
    "            accuracy_s, precision_s, recall_s, f1_s = eval_results(predicted_y_train, y_train, predicted_y_val, y_val, fold)\n",
    "\n",
    "            accuracy.append(accuracy_s)\n",
    "            precision.append(precision_s)\n",
    "            recall.append(recall_s)\n",
    "            f1.append(f1_s)\n",
    "\n",
    "            metrics_df = pd.DataFrame(\n",
    "                        {\n",
    "                            'accuracy': accuracy,\n",
    "                            'precision': precision,\n",
    "                            'recall':recall,\n",
    "                            'f1':f1\n",
    "                        }\n",
    "                    )\n",
    "            fold += 1\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests: max_depth : 1-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = ds.Table.read_table('Climate1SupportiveLevel.csv', sep=',')\n",
    "df2 = ds.Table.read_table('ClimateBalancedDS2.csv', sep=',')\n",
    "df = df1.append(df2)\n",
    "test_index = np.random.choice(df.num_rows, 100, replace=False)\n",
    "train_val_index = [i for i in np.arange(df.num_rows) if i not in test_index]\n",
    "test_data = df.take[test_index]\n",
    "df = df.take[train_val_index]\n",
    "X = list(df['Text'])\n",
    "y = list(df['Support'])\n",
    "test_X = list(test_data['Text'])\n",
    "test_y = list(test_data['Support'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### test1: stop-words: default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxdepth= 1\n",
      "test f1\n",
      "0.4742268041237113\n",
      "train f1\n",
      "0.6055692633063094\n",
      "maxdepth= 2\n",
      "test f1\n",
      "0.5945945945945946\n",
      "train f1\n",
      "0.6852791878172589\n",
      "maxdepth= 3\n",
      "test f1\n",
      "0.6194690265486726\n",
      "train f1\n",
      "0.6914826498422713\n",
      "maxdepth= 4\n",
      "test f1\n",
      "0.6194690265486726\n",
      "train f1\n",
      "0.6964454230890217\n",
      "maxdepth= 5\n",
      "test f1\n",
      "0.6194690265486726\n",
      "train f1\n",
      "0.7004086765168186\n",
      "maxdepth= 6\n",
      "test f1\n",
      "0.6194690265486726\n",
      "train f1\n",
      "0.7090570719602977\n"
     ]
    }
   ],
   "source": [
    "f1_lst_test1 = []\n",
    "f1_lst_train1 = []\n",
    "for d in range(1, 16):\n",
    "    k_fold_evaluate(X, y, max_depth=d, min_samples_leaf=2, stop_words='english', print_eval=False, overfit_risk=True)\n",
    "    clf_tmp = joblib.load('ClimateTeam7PD2_maxdepth{}.pkl'.format(d))\n",
    "    print('maxdepth=', d)\n",
    "    # test\n",
    "    y_pred = clf_tmp.predict(test_X)\n",
    "    print('test f1')\n",
    "    print(f1_score(y_pred=y_pred, y_true=test_y))\n",
    "    f1_lst_test1.append(f1_score(y_pred=y_pred, y_true=test_y))\n",
    "    # train_val\n",
    "    y_pred = clf_tmp.predict(X)\n",
    "    print('train f1')\n",
    "    print(f1_score(y_pred=y_pred, y_true=y))\n",
    "    f1_lst_train1.append(f1_score(y_pred=y_pred, y_true=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,16), f1_lst_test1, label='f1 of testset')\n",
    "plt.plot(np.arange(1,16), f1_lst_train1, label='f1 of trainset')\n",
    "plt.xlabel('max_depth')\n",
    "plt.title('Double check the overfitting risk')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### test2: stop_words: ['a', 'an', 'the', 'it', 'is', 'are', 'be', 'of', 'this', 'that', 'RT', 'rt','https']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "f1_lst_test2 = []\n",
    "f1_lst_train2 = []\n",
    "for d in range(1, 16):\n",
    "    k_fold_evaluate(X, y, max_depth=d, min_samples_leaf=2, \n",
    "                    stop_words=['a', 'an', 'the', 'it', 'is', 'are', 'be', 'of', 'this', 'that', 'RT', 'rt','https'], \n",
    "                    print_eval=False, overfit_risk=True)\n",
    "    clf_tmp = joblib.load('ClimateTeam7PD2_maxdepth{}.pkl'.format(d))\n",
    "    print('maxdepth=', d)\n",
    "    # test\n",
    "    y_pred = clf_tmp.predict(test_X)\n",
    "    print('test f1')\n",
    "    print(f1_score(y_pred=y_pred, y_true=test_y))\n",
    "    f1_lst_test2.append(f1_score(y_pred=y_pred, y_true=test_y))\n",
    "    # train_val\n",
    "    y_pred = clf_tmp.predict(X)\n",
    "    print('train f1')\n",
    "    print(f1_score(y_pred=y_pred, y_true=y))\n",
    "    f1_lst_train2.append(f1_score(y_pred=y_pred, y_true=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,16), f1_lst_test2, label='f1 of testset')\n",
    "plt.plot(np.arange(1,16), f1_lst_train2, label='f1 of trainset')\n",
    "plt.xlabel('max_depth')\n",
    "plt.title('Double check the overfitting risk')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test3: stop_words: most common words in DS1+DS2 tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_w = [i[0] for i in Counter([word for sentence in X for word in sentence.split() \n",
    "                                 if 'climate' not in word.lower() and 'change' not in word.lower()\n",
    "                                 and word.isalpha() \n",
    "                                 and len(word)>1]).most_common()[:30]]\n",
    "stop_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_lst_test3 = []\n",
    "f1_lst_train3 = []\n",
    "for d in range(1, 16):\n",
    "    k_fold_evaluate(X, y, max_depth=d, min_samples_leaf=2, \n",
    "                    stop_words=stop_w, \n",
    "                    print_eval=False, overfit_risk=True)\n",
    "    clf_tmp = joblib.load('ClimateTeam7PD2_maxdepth{}.pkl'.format(d))\n",
    "    print('maxdepth=', d)\n",
    "    # test\n",
    "    y_pred = clf_tmp.predict(test_X)\n",
    "    print('test f1')\n",
    "    print(f1_score(y_pred=y_pred, y_true=test_y))\n",
    "    f1_lst_test3.append(f1_score(y_pred=y_pred, y_true=test_y))\n",
    "    # train_val\n",
    "    y_pred = clf_tmp.predict(X)\n",
    "    print('train f1')\n",
    "    print(f1_score(y_pred=y_pred, y_true=y))\n",
    "    f1_lst_train3.append(f1_score(y_pred=y_pred, y_true=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,16), f1_lst_test3, label='f1 of testset')\n",
    "plt.plot(np.arange(1,16), f1_lst_train3, label='f1 of trainset')\n",
    "plt.xlabel('max_depth')\n",
    "plt.title('Double check the overfitting risk')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 8))\n",
    "metrics_df = k_fold_evaluate(X, y, max_depth=10, min_samples_leaf=2, \n",
    "                    stop_words=stop_w, \n",
    "                    print_eval=True, overfit_risk=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = joblib.load('ClimateTeam7PD2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = ds.Table.read_table('Climate1SupportiveLevel.csv', sep=',')\n",
    "df2 = ds.Table.read_table('ClimateBalancedDS2.csv', sep=',')\n",
    "df = df1.append(df2)\n",
    "X = list(df['Text'])\n",
    "y = list(df['Support'])\n",
    "y_pred = clf_tmp.predict(X)\n",
    "f1_score(y, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
