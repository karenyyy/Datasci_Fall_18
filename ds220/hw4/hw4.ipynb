{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Task 1\n",
    "\n",
    "Insert 4 vehicles from the State College Auto Database (can be the same ones you used in Redis) into the MongoDB.  One easy way to do this is to create a .json file from the data and then import the .json file into a new data collection using the 'mongoimport' package as you did in HW #3 to import the restaurant dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cars = pd.read_csv('cars.csv',sep=',',index_col=0)\n",
    "with open('cars.json', 'w') as outfile:\n",
    "    cars_dict = cars.to_dict('index')\n",
    "    for i in range(len(cars_dict)):\n",
    "        json.dump(cars_dict[i], outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-06T01:22:33.494-0500\tconnected to: localhost\n",
      "2018-11-06T01:22:34.035-0500\timported 10 documents\n"
     ]
    }
   ],
   "source": [
    "! mongoimport --db local --collection cars --file cars.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Task 2\n",
    "\n",
    "Query your database and demonstrate that you can search on at least 2 attributes of the vehicles. Demonstrate a query on a single attribute and a query on more than one attribute. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- query on a single attribute\n",
    "![](2.png)\n",
    "\n",
    "- query on more than one attribute\n",
    "![](3.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Task 3\n",
    "\n",
    "Demonstrate a query for a vehicle not in the database. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "\n",
    "What are differences you found between using MongoDB and Redis for loading and storing this data?  What would be a role for a database like Redis in storing this vehicle information? Can you imagine challenges in using the method you used for data import into MongoDB when dealing with Big Data?  How might you address any challenges in a big data environment? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With mongoDB we can import the whole dataset in the json format directly while in Redis, we have to import one key-value pair at a time. In this case, Redis acts like a dictionary in python used to store the nested dictionaries (key-value pairs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.toree.interpreter.broker.BrokerException\n",
       "Message: Py4JJavaError: An error occurred while calling o188.save.\n",
       ": java.lang.ClassNotFoundException: Failed to find data source: com.mongodb.spark.sql.DefaultSource. Please find packages at http://spark.apache.org/third-party-projects.html\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:546)\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSource.providingClass$lzycompute(DataSource.scala:87)\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSource.providingClass(DataSource.scala:87)\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:467)\n",
       "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:50)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:609)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:233)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:280)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "Caused by: java.lang.ClassNotFoundException: com.mongodb.spark.sql.DefaultSource.DefaultSource\n",
       "\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n",
       "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n",
       "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$22$$anonfun$apply$14.apply(DataSource.scala:530)\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$22$$anonfun$apply$14.apply(DataSource.scala:530)\n",
       "\tat scala.util.Try$.apply(Try.scala:192)\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$22.apply(DataSource.scala:530)\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$22.apply(DataSource.scala:530)\n",
       "\tat scala.util.Try.orElse(Try.scala:84)\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:530)\n",
       "\t... 28 more\n",
       "\n",
       "(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError('An error occurred while calling o188.save.\\n', JavaObject id=o189), <traceback object at 0x7f5b7a040d08>)\n",
       "StackTrace: org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "scala.Option.foreach(Option.scala:257)\n",
       "org.apache.toree.interpreter.broker.BrokerState.markFailure(BrokerState.scala:162)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:498)\n",
       "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
       "py4j.Gateway.invoke(Gateway.java:280)\n",
       "py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
       "java.lang.Thread.run(Thread.java:748)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Row, functions\n",
    "\n",
    "def parseInput(line):\n",
    "    fields = line.split('|')\n",
    "    return Row(user_id = int(fields[0]), \n",
    "               age = int(fields[1]),\n",
    "               gender = fields[2],\n",
    "               occupation = fields[3],\n",
    "               zip = fields[4])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    spark = SparkSession.builder.appName('MongoDBIntegration').getOrCreate()\n",
    "    lines = spark.sparkContext.textFile('hdfs://localhost:9002/user/data/u.user')\n",
    "    users = lines.map(parseInput)\n",
    "    usersDataset = spark.createDataFrame(users)\n",
    "    \n",
    "    # write into MongoDB\n",
    "    usersDataset.write \\\n",
    "                .format('com.mongodb.spark.sql.DefaultSource') \\\n",
    "                .option('uri', 'mongodb://localhost:27017/movie.users') \\\n",
    "                .mode('append') \\\n",
    "                .save()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - PySpark",
   "language": "python",
   "name": "apache_toree_pyspark"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "pygments_lexer": "python",
   "version": "3.6.6\n"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
