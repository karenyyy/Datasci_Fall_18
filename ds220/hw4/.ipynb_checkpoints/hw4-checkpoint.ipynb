{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Task 1\n",
    "\n",
    "Insert 4 vehicles from the State College Auto Database (can be the same ones you used in Redis) into the MongoDB.  One easy way to do this is to create a .json file from the data and then import the .json file into a new data collection using the 'mongoimport' package as you did in HW #3 to import the restaurant dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cars = pd.read_csv('cars.csv',sep=',',index_col=0)\n",
    "with open('cars.json', 'w') as outfile:\n",
    "    cars_dict = cars.to_dict('index')\n",
    "    for i in range(len(cars_dict)):\n",
    "        json.dump(cars_dict[i], outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-06T01:22:33.494-0500\tconnected to: localhost\n",
      "2018-11-06T01:22:34.035-0500\timported 10 documents\n"
     ]
    }
   ],
   "source": [
    "! mongoimport --db local --collection cars --file cars.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Task 2\n",
    "\n",
    "Query your database and demonstrate that you can search on at least 2 attributes of the vehicles. Demonstrate a query on a single attribute and a query on more than one attribute. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- query on a single attribute\n",
    "![](2.png)\n",
    "\n",
    "- query on more than one attribute\n",
    "![](3.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Task 3\n",
    "\n",
    "Demonstrate a query for a vehicle not in the database. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "\n",
    "What are differences you found between using MongoDB and Redis for loading and storing this data?  What would be a role for a database like Redis in storing this vehicle information? Can you imagine challenges in using the method you used for data import into MongoDB when dealing with Big Data?  How might you address any challenges in a big data environment? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With mongoDB we can import the whole dataset in the json format directly while in Redis, we have to import one key-value pair at a time. In this case, Redis acts like a dictionary in python used to store the nested dictionaries (key-value pairs). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- challenges:\n",
    "    - the dataset to insert is too large\n",
    "    - take up too much memory\n",
    "    - it takes too long to import the whole dataset\n",
    "    \n",
    "    \n",
    "- possible solutions to address the challenges:\n",
    "    - import the data in batches\n",
    "        - mongoimport --db local --collection cars --file cars.json --batchSize 1\n",
    "    - disable journaling to avoid further memory consumption when tracking the changes made\n",
    "        - mongod --nojournal\n",
    "    - import the dataset in parallel with `--numInsertionWorkers`\n",
    "        - mongoimport --db local --collection cars --file cars.json --numInsertionWorkers 8\n",
    "    - convert text data into bson format\n",
    "    - use GridFS when the file exceeds the size limit of 16MB\n",
    "        - \n",
    "    - if there are complex analyses on the data involved, such as apply a machine learning algorithm to do classification, a possible way is to use mongodb-spark-connector to analyze with spark mllib, then import dataset from hdfs back to mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -put cars.csv /user/data\n",
    "pyspark --packages org.mongodb.spark:mongo-spark-connector_2.11:2.2.1\n",
    "        --packages org.mongodb:mongo-java-driver:3.8.0\n",
    "        --packages org.apache.spark:spark-sql_2.11:2.3.0\n",
    "        --packages com.stratio.datasource:spark-mongodb_2.11:0.12.0\n",
    "        --packages org.mongodb:casbah_2.11:3.0.0\n",
    "        --packages org.apache.spark:spark-catalyst_2.11:2.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row, functions\n",
    "\n",
    "def parseInput(line):\n",
    "    fields = line.split(',')\n",
    "    return Row(Title = fields[1], \n",
    "               Year = fields[2],\n",
    "               Mileage = fields[3],\n",
    "               Make = fields[4],\n",
    "               Model = fields[5],\n",
    "               Trim = fields[6],\n",
    "               Style = fields[7],\n",
    "               Engine = fields[8],\n",
    "               Exterior_Color = fields[9],\n",
    "               Interior_Color = fields[10],\n",
    "               VIN = fields[11],\n",
    "               Stock = fields[12],\n",
    "               description = fields[13])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    spark = SparkSession.builder.appName('MongoDBIntegration').getOrCreate()\n",
    "    lines = spark.sparkContext.textFile('hdfs://localhost:9002/user/data/cars.json')\n",
    "    users = lines.map(parseInput)\n",
    "    usersDataset = spark.createDataFrame(users)\n",
    "    \n",
    "    # write into MongoDB\n",
    "    usersDataset.write \\\n",
    "                .format('com.stratio.datasource.mongodb') \\\n",
    "                .options(host=\"localhost:27017\", database=\"local\", collection=\"cars2\") \\\n",
    "                .mode('append') \\\n",
    "                .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](6.png)\n",
    "\n",
    "![](7.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
